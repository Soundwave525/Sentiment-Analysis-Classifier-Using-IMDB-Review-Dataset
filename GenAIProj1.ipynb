{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "341ae0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cdaf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. LOAD IMDB DATASET\n",
    "# (Folder structure: aclImdb/train/pos, aclImdb/train/neg)\n",
    "# ============================================================\n",
    "\n",
    "train_data = load_files(\"aclImdb/train\", categories=['pos','neg'])\n",
    "test_data = load_files(\"aclImdb/test\", categories=['pos','neg'])\n",
    "\n",
    "X_train, y_train = train_data.data, train_data.target\n",
    "X_test, y_test = test_data.data, test_data.target\n",
    "\n",
    "# Convert bytes to string\n",
    "X_train = [doc.decode(\"utf-8\", errors=\"ignore\") for doc in X_train]\n",
    "X_test = [doc.decode(\"utf-8\", errors=\"ignore\") for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72320709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. TEXT PREPROCESSING\n",
    "# Lowercase + punctuation removal + stopwords + lemmatization\n",
    "# ============================================================\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word not in stop_words and word.isalpha()\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X_train_clean = [preprocess(doc) for doc in X_train]\n",
    "X_test_clean = [preprocess(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27907b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. TF-IDF WITH N-GRAMS (1,2)\n",
    "# ============================================================\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_clean)\n",
    "X_test_tfidf = tfidf.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8419a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. WORD2VEC EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "tokenized_train = [doc.split() for doc in X_train_clean]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_train,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "def document_vector(tokens):\n",
    "    vectors = [\n",
    "        w2v_model.wv[word]\n",
    "        for word in tokens\n",
    "        if word in w2v_model.wv\n",
    "    ]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "X_train_w2v = np.array([document_vector(doc.split()) for doc in X_train_clean])\n",
    "X_test_w2v = np.array([document_vector(doc.split()) for doc in X_test_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48a684e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. DEFINE CLASSIFIERS\n",
    "# ============================================================\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"Naive Bayes\": MultinomialNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d88cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. EVALUATION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def evaluate(model, X_train, X_test, y_train, y_test, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Model:\", name)\n",
    "    print(\"==============================\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\\n\")\n",
    "    print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. TRAIN & EVALUATE ON TF-IDF FEATURES\n",
    "# ============================================================\n",
    "\n",
    "for name, model in models.items():\n",
    "    evaluate(model, X_train_tfidf, X_test_tfidf, y_train, y_test, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. TRAIN & EVALUATE ON WORD2VEC FEATURES\n",
    "# (Skipping Naive Bayes for Word2Vec)\n",
    "# ============================================================\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"Naive Bayes\":\n",
    "        continue\n",
    "    evaluate(model, X_train_w2v, X_test_w2v, y_train, y_test, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
